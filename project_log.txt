[1] Mastering Diverse Domains through World Models - 2023
D. Hafner, J. Pasukonis, J. Ba, T. Lillicrap
https://arxiv.org/pdf/2301.04104v1.pdf


2023/01/29:
    - Implemented Symlog function and inverse symlog function. Nice to do the math ourselves:
        # To get to symlog inverse, we solve the symlog equation for x:
        #     y = sign(x) * log(|x| + 1)
        # <=> y / sign(x) = log(|x| + 1)
        # <=> y =  log( x + 1) V x >= 0
        #    -y =  log(-x + 1) V x <  0
        # <=> exp(y)  =  x + 1  V x >= 0
        #     exp(-y) = -x + 1  V x <  0
        # <=> exp(y)  - 1 =  x   V x >= 0
        #     exp(-y) - 1 = -x   V x <  0
        # <=>  exp(y)  - 1 = x   V x >= 0 (if x >= 0, then y must also be >= 0)
        #     -exp(-y) - 1 = x   V x <  0 (if x < 0, then y must also be < 0)
        # <=> sign(y) * (exp(|y|) - 1) = x
    - Started working on Atari CNN for encoder:
        Difficulty: Understanding appendix B (table of hyperparams): How do I build
        size=XS CNN? says: multiplier=24, strides=(2,2), stop (flatten) after image
        has been reduced to dimensions 4x4, but what kernel should we use? If we use 4x4
        (input=64x64x3, strides=2,2 filters=24,48,96) then the final image size would be 6x6x96

2023/01/30:
    - Completed Atari encoder CNN.
        Found answer to above question in appendix C: Summary of differences:
        kernel=3 (not 4 like in Schmidhuber's world models) and padding="same" (not "valid")
    - Implemted z-generating sub-network.
        Figured out the difficulty of sampling from a categorical distribution and
        keeping the result differentiable (Dreamer V2 paper: Algorithm 1).

2023/01/31:
    - Completed Atari decoder CNNTranspose.
        - Difficulties: How to reverse a CNN stack. Use same padding, same kernel, same strides.
        - Start from reshaped (4x4x[final num CNN filters]) output of a dense layer and feed
          that into the first conv2dtranspose.
        - The final output are the means of a 64x64x3 diag-Gaussian with std=1.0, from which
          we sample the final image.
    - Completed Sequence (GRU) model.
    - Completed Dynamics Predictor, using z-generator and MLP.
    - Implemented two-hot encoding.
        Question: What's the range used of the 255 buckets
            Answer: -20 to +20 (since the range is for the symlog'd values, -20 to 20
            covers almost any possible reward range)
        Difficulty: What's the most performant way for the implementation?
            answer: Using floor, ceil, tf.scatter_nd()
    - Implemented Reward-predictor and continue-predictor.

2023/02/01
    - Complete Atari World Model code (roughly, w/o actually running it).
    - Complete World Model loss functions (L_pred, L_dyn, L_rep) (roughly w/o
      actually running it).
    - Lift existing EnvRunner from other project ("Attention is all you need") and adjust
      such that:
        - It can run on vectorized Atari envs using gymnasium.vector.make()
        - It produces samples (obs, actions, rewards, trunc., term., valid-masks) of
          shape (B=num-sub-envs, T=max-seq-len, ...)
          Note that T does NOT go beyond episode boundaries. If an episode ends within
          a current T-rollout, the remaining max-seq-len - len timesteps are filled with
          zero values. The returned mask indicates, which exact B,T positions are valid
          and which aren't and thus can be used to properly mask the total loss
          computation.
        - Next obs are NOT fully (B, T)-provided to save space. They are only ever
          needed in case of a truncated rollout (max-seq-len) or the episode is truncated
          in the middle of the rollout (before T is reached). Thus next_obs are returned
          as (B, dim), NOT as (B, T, dim)
        - Returned samples could be inserted as is (after splitting along the B-axis)
          into a Dreamer-style replay buffer with max-seq-len being the "batch len"
          described in the paper.

2023/02/02
    - Finished fixing bugs in EnvRunner; wrote a small test script for collecting
      vectorized Pacman samples.
    - Wrote quick non-RLlib training script for a AtariWorlModel using the above simple
      EnvRunner; this does not include a buffer yet, but has an Atari world model,
      optimizer, calls loss functions and updates the model.
    - Started debugging World Model training code by stepping through the above example
      script and weeding out errors in the code. For example:
        - output of decoder is NOT a Normal, but a MultivariateNormalDiag (each pixel
          is an independent variable).
        - weights of reward-bucket dense layer must be zero-initialized according to the
          paper.
    - Got the entire sampling + loss + updating loop to run for the world model.
    - TODO: Continue debugging and successfully train.

    Open questions:
    - When symlog'ing Atari image observations, do we brutally just pass in the 0-255 uint8
      values into the symlog function? Or do we - even though we have symlog - still have
      to normalize observations?
    - For the binned reward distributions, which tf distribution should we take?
        - Should sampling always return exact bin values (or also values in between,
          like in a truly continuous distr.)?
        - How to compute log-probs from a given (possibly not-exactly-on-the-bin-value
          real reward)?
          Answer: Use FiniteDiscrete with `atol`==bucket_delta / 2.0.

2023/02/03
    - Answered question on how to compute the KL-divergence for two different
      z-distributions. Each z-distribution is a 32xCategorical (over 32 classes each)
      distribution. We could use the tfp Independent distribution for that with
      args:
        `distribution=Categorical(probs=[B, num_categoricals, num_classes])`
        `reinterpreted_batch_ndims=1` <- dim 1 above are the independent categoricals
      KL is computed as a sum(!) (not mean) of the individual distributions' KLs.
      Note: We do the same in RLlib for our multi-action distribution
    - Write 30 lines replay buffer.
    - Got example to run and learn a little (commit 1cf47fecd32a3c2ca1cf7aa757465f3856b26710).
        - Have to fix the training ratio (1024) and the model size (S instead of currently XS).
    - Implemented `model_dimension` parameter for all components, such that the world-model
      can be constructed with a simple scaling parameter: "XS", "S", ... (see [1] appendix B)
    - Implemented unimix categoricals for world model representation layer and dynamics
      predictor.

    - Open questions:
        According to the paper: "DreamerV2 used a replay buffer that only replays time
        steps from completed episodes. To shorten the feedback loop, DreamerV3 uniformly
        samples from all inserted subsequences of size batch length regardless of
        episode boundaries." -> Need to probably fix our env-runner to return (non-masked)
        sequences, which may cross episode boundaries. This would solve the problem of some
        batches having smaller losses due to masking some of the timesteps, but introduce the
        new problem of having to GAE (for critic/actor loss) within the same sequence.
    - According to eq. 5 in [1], the prediction loss is solely using neg-log(p) style loss terms.
      However, it also says in Appendix C: "Symlog predictions: We symlog encode inputs to
      the world model and use symlog predictions with **squared error** for reconstructing
      inputs ...". This seems contradictory.
    - How do we store h(0) in the buffer? We need this to start the z-computations (and subsequent
      h-computations).
      a) Store the initial_h along with the trajectories
      (you currently store the last(!) h instead and use that as the initial one for the
      same sequence, which is completely wrong).
      b) Update the initial_h of sequence B inside the buffer, iff we just made a training
      pass through sequence A, whose end is the beginning of sequence B.
      c) Don't store anything and always start with zero-vector.


OPEN PROBLEMS:
- Figure out reason of actor loss:
- Implement unimix categoricals for actor network (for discrete actions?)
- For now, I have been training deterministic Atari only:
  no sticky actions (action_repeat_probability=0.0), and frameskip=4
