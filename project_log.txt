2023/01/29:
    - Implemented Symlog function and inverse symlog function. Nice to do the math ourselves:
        # To get to symlog inverse, we solve the symlog equation for x:
        #     y = sign(x) * log(|x| + 1)
        # <=> y / sign(x) = log(|x| + 1)
        # <=> y =  log( x + 1) V x >= 0
        #    -y =  log(-x + 1) V x <  0
        # <=> exp(y)  =  x + 1  V x >= 0
        #     exp(-y) = -x + 1  V x <  0
        # <=> exp(y)  - 1 =  x   V x >= 0
        #     exp(-y) - 1 = -x   V x <  0
        # <=>  exp(y)  - 1 = x   V x >= 0 (if x >= 0, then y must also be >= 0)
        #     -exp(-y) - 1 = x   V x <  0 (if x < 0, then y must also be < 0)
        # <=> sign(y) * (exp(|y|) - 1) = x
    - Started working on Atari CNN for encoder:
        Difficulty: Understanding appendix B (table of hyperparams): How do I build
        size=XS CNN? says: multiplier=24, strides=(2,2), stop (flatten) after image
        has been reduced to dimensions 4x4, but what kernel should we use? If we use 4x4
        (input=64x64x3, strides=2,2 filters=24,48,96) then the final image size would be 6x6x96

2023/01/30:
    - Completed Atari encoder CNN.
        Found answer to above question in appendix C: Summary of differences:
        kernel=3 (not 4 like in Schmidhuber's world models) and padding="same" (not "valid")
    - Implemted z-generating sub-network.
        Figured out the difficulty of sampling from a categorical distribution and
        keeping the result differentiable (Dreamer V2 paper: Algorithm 1).

2023/01/31:
    - Completed Atari decoder CNNTranspose.
        - Difficulties: How to reverse a CNN stack. Use same padding, same kernel, same strides.
        - Start from reshaped (4x4x[final num CNN filters]) output of a dense layer and feed
          that into the first conv2dtranspose.
        - The final output are the means of a 64x64x3 diag-Gaussian with std=1.0, from which
          we sample the final image.
    - Completed Sequence (GRU) model.
    - Completed Dynamics Predictor, using z-generator and MLP.
    - Implemented two-hot encoding.
        Open question: What's the range used of the 255 buckets? -20 to +20?
        Difficulty: What's the most performant way for the implementation?
            answer: Using floor, ceil, tf.scatter_nd()
    - Implemented Reward-predictor and continue-predictor.

2023/02/01
    - Complete Atari World Model code (roughly, w/o actually running it).
    - Complete World Model loss functions (L_pred, L_dyn, L_rep) (roughly w/o
      actually running it).
    - Lift existing EnvRunner from other project ("Attention is all you need") and adjust
      such that:
        - It can run on vectorized Atari envs using gymnasium.vector.make()
        - It produces samples (obs, actions, rewards, trunc., term., valid-masks) of
          shape (B=num-sub-envs, T=max-seq-len, ...)
          Note that T does NOT go beyond episode boundaries. If an episode ends within
          a current T-rollout, the remaining max-seq-len - len timesteps are filled with
          zero values. The returned mask indicates, which exact B,T positions are valid
          and which aren't and thus can be used to properly mask the total loss
          computation.
        - Next obs are NOT fully (B, T)-provided to save space. They are only ever
          needed in case of a truncated rollout (max-seq-len) or the episode is truncated
          in the middle of the rollout (before T is reached). Thus next_obs are returned
          as (B, dim), NOT as (B, T, dim)
        - Returned samples could be inserted as is (after splitting along the B-axis)
          into a Dreamer-style replay buffer with max-seq-len being the "batch len"
          described in the paper.

2023/02/02
    - Finished fixing bugs in EnvRunner; wrote a small test script for collecting
      vectorized Pacman samples.
    - Wrote quick non-RLlib training script for a AtariWorlModel using the above simple
      EnvRunner; this does not include a buffer yet, but has an Atari world model,
      optimizer, calls loss functions and updates the model.
    - Started debugging World Model training code by stepping through the above example
      script and weeding out errors in the code. For example:
        - output of decoder is NOT a Normal, but a MultivariateNormalDiag (each pixel
          is an independent variable).
        - weights of reward-bucket dense layer must be zero-initialized according to the
          paper.
    - Got the entire sampling + loss + updating loop to run.
    - TODO: Continue debugging and successfully train.

    Open questions:
    - When symlog'ing Atari image observations, do we brutally just pass in the 0-255 uint8
      values into the symlog function? Or do we - even though we have symlog - still have
      to normalize observations?
    - For the binned reward distributions, which tf distribution should we take?
        - Should sampling always return exact bin values (or also values in between,
          like in a truly continuous distr.)?
        - How to compute log-probs from a given (possibly not-exactly-on-the-bin-value
          real reward)?
          Answer: Use FiniteDiscrete with `atol`==bucket_delta / 2.0.

OPEN PROBLEMS:
- mask returned by EnvRunner is wrong (still logic from attention net, where mask
  has a different task (mask future timesteps for self-attention that model cannot
  know (yet))).
